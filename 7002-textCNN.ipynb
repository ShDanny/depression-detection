{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b65c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# save models\n",
    "import numpy as np # TF-IDF \n",
    "import pickle\n",
    "import jieba \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4157a",
   "metadata": {},
   "source": [
    "# train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f69b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27693417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd74dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf55102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_0 = df2[df2['label'] == 0].sample(n=1000, random_state=43)\n",
    "df_label_1 = df2[df2['label'] == 1].sample(n=1000, random_state=43)\n",
    "\n",
    "# 合并两个dataframe\n",
    "df3 = pd.concat([df_label_0, df_label_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbf31430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3=df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8211fc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10148</th>\n",
       "      <td>31839</td>\n",
       "      <td>0</td>\n",
       "      <td>手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>22295</td>\n",
       "      <td>0</td>\n",
       "      <td>北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>17168</td>\n",
       "      <td>0</td>\n",
       "      <td>早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>21714</td>\n",
       "      <td>0</td>\n",
       "      <td>式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>29150</td>\n",
       "      <td>0</td>\n",
       "      <td>做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>1163</td>\n",
       "      <td>1</td>\n",
       "      <td>真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14887</th>\n",
       "      <td>4563</td>\n",
       "      <td>1</td>\n",
       "      <td>清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>1672</td>\n",
       "      <td>1</td>\n",
       "      <td>生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>3860</td>\n",
       "      <td>1</td>\n",
       "      <td>花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18474</th>\n",
       "      <td>8150</td>\n",
       "      <td>1</td>\n",
       "      <td>习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "10148  31839      0  手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...\n",
       "1692   22295      0  北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...\n",
       "2692   17168      0  早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...\n",
       "7703   21714      0  式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...\n",
       "321    29150      0  做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...\n",
       "...      ...    ...                                                ...\n",
       "11487   1163      1  真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...\n",
       "14887   4563      1  清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...\n",
       "11996   1672      1  生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...\n",
       "14184   3860      1  花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...\n",
       "18474   8150      1  习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09efa08",
   "metadata": {},
   "source": [
    "# 新训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15340a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 [==============================] - 45s 1s/step - loss: 0.6553 - accuracy: 0.6443 - val_loss: 0.5530 - val_accuracy: 0.9050\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 42s 962ms/step - loss: 0.3631 - accuracy: 0.9150 - val_loss: 0.2313 - val_accuracy: 0.9233\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 42s 960ms/step - loss: 0.1827 - accuracy: 0.9243 - val_loss: 0.1769 - val_accuracy: 0.9283\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 42s 961ms/step - loss: 0.1295 - accuracy: 0.9443 - val_loss: 0.1658 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 42s 956ms/step - loss: 0.0934 - accuracy: 0.9657 - val_loss: 0.1490 - val_accuracy: 0.9417\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 42s 957ms/step - loss: 0.0603 - accuracy: 0.9829 - val_loss: 0.1405 - val_accuracy: 0.9450\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 42s 961ms/step - loss: 0.0374 - accuracy: 0.9921 - val_loss: 0.1390 - val_accuracy: 0.9483\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 42s 962ms/step - loss: 0.0257 - accuracy: 0.9950 - val_loss: 0.1395 - val_accuracy: 0.9450\n",
      "Epoch 9/10\n",
      "44/44 [==============================] - 42s 958ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.1409 - val_accuracy: 0.9450\n",
      "Epoch 10/10\n",
      "44/44 [==============================] - 42s 964ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.1442 - val_accuracy: 0.9467\n",
      "19/19 [==============================] - 4s 208ms/step\n",
      "Accuracy: 0.9466666666666667\n",
      "F1 Score: 0.9452054794520547\n",
      "Recall: 0.9261744966442953\n",
      "Precision: 0.965034965034965\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = tf.keras.layers.Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n",
    "    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "concatenated = tf.keras.layers.Concatenate()(conv_layers)\n",
    "dropout = tf.keras.layers.Dropout(0.5)(concatenated)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c477a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test200\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test200\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "tf.saved_model.save(model, 'TextCNN_0807test200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1116cb",
   "metadata": {},
   "source": [
    "# f20230807 解决save model问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.637 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "989d8892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 59s 2s/step - loss: 0.6636 - accuracy: 0.6450 - val_loss: 0.5937 - val_accuracy: 0.9139\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4295 - accuracy: 0.9175 - val_loss: 0.2755 - val_accuracy: 0.9211\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.1680 - accuracy: 0.9408 - val_loss: 0.1688 - val_accuracy: 0.9311\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.1102 - accuracy: 0.9525 - val_loss: 0.1502 - val_accuracy: 0.9368\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 54s 1s/step - loss: 0.0761 - accuracy: 0.9742 - val_loss: 0.1466 - val_accuracy: 0.9400\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 62s 2s/step - loss: 0.0502 - accuracy: 0.9883 - val_loss: 0.1376 - val_accuracy: 0.9418\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 58s 2s/step - loss: 0.0374 - accuracy: 0.9917 - val_loss: 0.1329 - val_accuracy: 0.9432\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 57s 2s/step - loss: 0.0218 - accuracy: 0.9958 - val_loss: 0.1369 - val_accuracy: 0.9414\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 58s 2s/step - loss: 0.0141 - accuracy: 0.9983 - val_loss: 0.1388 - val_accuracy: 0.9418\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 56s 1s/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.1358 - val_accuracy: 0.9446\n",
      "88/88 [==============================] - 21s 233ms/step\n",
      "Accuracy: 0.9446428571428571\n",
      "F1 Score: 0.9435748088824172\n",
      "Recall: 0.9303661162957645\n",
      "Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = tf.keras.layers.Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n",
    "    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "concatenated = tf.keras.layers.Concatenate()(conv_layers)\n",
    "dropout = tf.keras.layers.Dropout(0.5)(concatenated)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a09c8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test2003\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test2003\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model with explicit tracing\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=(None, max_sequence_length), dtype=tf.int32)])\n",
    "def predict_fn(x):\n",
    "    return model(x)\n",
    "\n",
    "tf.saved_model.save(model, 'TextCNN_0807test2003', signatures={'serving_default': predict_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8b36f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model using model.save()\n",
    "model.save('TextCNN_0807test2007.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c6a153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 19s 212ms/step\n",
      "Loaded Model Accuracy: 0.9446428571428571\n",
      "Loaded Model F1 Score: 0.9435748088824172\n",
      "Loaded Model Recall: 0.9303661162957645\n",
      "Loaded Model Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('TextCNN_0807test2007.h5')\n",
    "\n",
    "# Evaluate the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_padded)\n",
    "y_pred_binary_loaded = (y_pred_loaded > 0.5).astype(int)\n",
    "\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\n",
    "f1_loaded = f1_score(y_test, y_pred_binary_loaded)\n",
    "recall_loaded = recall_score(y_test, y_pred_binary_loaded)\n",
    "precision_loaded = precision_score(y_test, y_pred_binary_loaded)\n",
    "\n",
    "print(\"Loaded Model Accuracy:\", accuracy_loaded)\n",
    "print(\"Loaded Model F1 Score:\", f1_loaded)\n",
    "print(\"Loaded Model Recall:\", recall_loaded)\n",
    "print(\"Loaded Model Precision:\", precision_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782a811",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb9dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c11cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b154173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')\n",
    "df2=balanced_df_all_cleaned_tokenization.copy()\n",
    "df_label_0 = df2[df2['label'] == 0].sample(n=2000, random_state=42)\n",
    "df_label_1 = df2[df2['label'] == 1].sample(n=2000, random_state=42)\n",
    "\n",
    "# 合并两个dataframe\n",
    "df3 = pd.concat([df_label_0, df_label_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e8f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10148</th>\n",
       "      <td>31839</td>\n",
       "      <td>0</td>\n",
       "      <td>手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>22295</td>\n",
       "      <td>0</td>\n",
       "      <td>北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>17168</td>\n",
       "      <td>0</td>\n",
       "      <td>早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>21714</td>\n",
       "      <td>0</td>\n",
       "      <td>式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>29150</td>\n",
       "      <td>0</td>\n",
       "      <td>做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>1163</td>\n",
       "      <td>1</td>\n",
       "      <td>真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14887</th>\n",
       "      <td>4563</td>\n",
       "      <td>1</td>\n",
       "      <td>清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>1672</td>\n",
       "      <td>1</td>\n",
       "      <td>生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>3860</td>\n",
       "      <td>1</td>\n",
       "      <td>花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18474</th>\n",
       "      <td>8150</td>\n",
       "      <td>1</td>\n",
       "      <td>习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "10148  31839      0  手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...\n",
       "1692   22295      0  北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...\n",
       "2692   17168      0  早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...\n",
       "7703   21714      0  式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...\n",
       "321    29150      0  做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...\n",
       "...      ...    ...                                                ...\n",
       "11487   1163      1  真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...\n",
       "14887   4563      1  清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...\n",
       "11996   1672      1  生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...\n",
       "14184   3860      1  花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...\n",
       "18474   8150      1  习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9c4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)#0.2 for test, 0.3 for whole\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb704b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any float values in X_train and X_test to strings\n",
    "X_train = [str(x) for x in X_train]\n",
    "X_test = [str(x) for x in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1216e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.775 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2e2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 219ms/step\n",
      "Loaded Model Accuracy: 0.9446428571428571\n",
      "Loaded Model F1 Score: 0.9435748088824172\n",
      "Loaded Model Recall: 0.9303661162957645\n",
      "Loaded Model Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('TextCNN_0807test2007.h5')\n",
    "\n",
    "# Evaluate the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_padded)\n",
    "y_pred_binary_loaded = (y_pred_loaded > 0.5).astype(int)\n",
    "\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\n",
    "f1_loaded = f1_score(y_test, y_pred_binary_loaded)\n",
    "recall_loaded = recall_score(y_test, y_pred_binary_loaded)\n",
    "precision_loaded = precision_score(y_test, y_pred_binary_loaded)\n",
    "\n",
    "print(\"Loaded Model Accuracy:\", accuracy_loaded)\n",
    "print(\"Loaded Model F1 Score:\", f1_loaded)\n",
    "print(\"Loaded Model Recall:\", recall_loaded)\n",
    "print(\"Loaded Model Precision:\", precision_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c60ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 7000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 7000, 100)    16086500    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 6998, 100)    30100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 6997, 100)    40100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 6996, 100)    50100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 100)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 100)         0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_5 (Global  (None, 100)         0           ['conv1d_5[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 300)          0           ['global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]', \n",
      "                                                                  'global_max_pooling1d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            301         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,207,101\n",
      "Trainable params: 16,207,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586fbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27dfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load('TextCNN_0705whole')# TextCNN_whole2 TextCNN_test TextCNN_0705whole\n",
    "\n",
    "# Convert the loaded model to a Keras model\n",
    "model = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cceab32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model 0807\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load('TextCNN_0807test2002')# TextCNN_whole2 TextCNN_test TextCNN_0705whole\n",
    "\n",
    "# Convert the loaded model to a Keras model\n",
    "model = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6323981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for evaluation\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6313fc07",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute __inference_signature_wrapper_6441 as input #0(zero-based) was expected to be a int32 tensor but is a float tensor [Op:__inference_signature_wrapper_6441]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m           return self._call_with_structured_signature(args, kwargs,\n\u001b[0m\u001b[0;32m   1614\u001b[0m                                                       cancellation_manager)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_structured_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         self._function_spec.canonicalize_function_inputs(*args, **kwargs))\n\u001b[1;32m-> 1691\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structured_signature_check_missing_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1692\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structured_signature_check_unexpected_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_structured_signature_check_missing_args\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1709\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmissing_arguments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1710\u001b[1;33m       raise TypeError(f\"{self._structured_signature_summary()} missing \"\n\u001b[0m\u001b[0;32m   1711\u001b[0m                       \u001b[1;34m\"required arguments: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: signature_wrapper(*, x) missing required arguments: x.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9076\\3258697868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbatch_data_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dense'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_output\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred_binary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1602\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \"\"\"\n\u001b[1;32m-> 1604\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1617\u001b[1;33m             return self._call_with_flat_signature(args, kwargs,\n\u001b[0m\u001b[0;32m   1618\u001b[0m                                                   cancellation_manager)\n\u001b[0;32m   1619\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1669\u001b[0m                         \u001b[1;34mf\"#{i}(zero-based) to be a Tensor; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m                         f\"got {type(arg).__name__} ({arg}).\")\n\u001b[1;32m-> 1671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# cross-replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_unused_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n\u001b[0m\u001b[0;32m    139\u001b[0m                                                     cancellation_manager)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute __inference_signature_wrapper_6441 as input #0(zero-based) was expected to be a int32 tensor but is a float tensor [Op:__inference_signature_wrapper_6441]"
     ]
    }
   ],
   "source": [
    "# Evaluate the model in batches\n",
    "y_pred_binary = []\n",
    "num_batches = int(np.ceil(len(X_test_padded) / batch_size))\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(X_test_padded))\n",
    "    batch_data = X_test_padded[start_idx:end_idx]\n",
    "    batch_data_tensor = tf.constant(batch_data, dtype=tf.float32)\n",
    "    batch_output = model(batch_data_tensor)['dense']\n",
    "    batch_pred = (batch_output > 0.5).numpy().astype(int)\n",
    "    y_pred_binary.extend(batch_pred)\n",
    "\n",
    "y_pred_binary = np.array(y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy_TextCNN = accuracy_score(y_test, y_pred_binary)\n",
    "f1_TextCNN = f1_score(y_test, y_pred_binary)\n",
    "recall_TextCNN = recall_score(y_test, y_pred_binary)\n",
    "precision_TextCNN = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_TextCNN)\n",
    "print(\"F1 Score:\", f1_TextCNN)\n",
    "print(\"Recall:\", recall_TextCNN)\n",
    "print(\"Precision:\", precision_TextCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec01a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [[0]]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "# new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "new_tweet_content = ['思诺思 舒乐安定 代开 抑郁 抑郁 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "new_tweet_sequences = [tokenize(tweet) for tweet in new_tweet_content]\n",
    "new_tweet_sequences = tokenizer.texts_to_sequences(new_tweet_sequences)\n",
    "new_tweet_padded = pad_sequences(new_tweet_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert the padded sequences to float32\n",
    "new_tweet_padded = new_tweet_padded.astype(np.float32)\n",
    "\n",
    "# Make predictions\n",
    "output = model(tf.constant(new_tweet_padded))  # Pass the data as a constant tensor\n",
    "predictions = output['dense']  # Assuming the final layer in your model is named 'dense'\n",
    "predicted_labels = (predictions > 0.5).numpy().astype(int)  # Convert the predictions to a numpy array\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e28c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f17ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
