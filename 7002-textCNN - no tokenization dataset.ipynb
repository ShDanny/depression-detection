{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b65c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# save models\n",
    "import numpy as np # TF-IDF \n",
    "import pickle\n",
    "import jieba \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4157a",
   "metadata": {},
   "source": [
    "# train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f69b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fff368e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25640</td>\n",
       "      <td>0</td>\n",
       "      <td>害真挺吃颜想岁足表达减肥决心姐妹谈恋爱感觉全世界闺蜜介绍成功行淦近新情侣装评手评寻思俩关系挺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28303</td>\n",
       "      <td>0</td>\n",
       "      <td>拥抱世界暖心动作生活中难免挫折坎坷安慌乱许拥抱充满量世界抗衡愿生山树栖心爱春赏花夏纳凉秋登山...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10546</td>\n",
       "      <td>0</td>\n",
       "      <td>雨天穿滑倒坐水里亲测艾特子赟家意见麻雀妈妈问麻雀扎头发麻雀说啾啾晚子赟帮辫头发感找年糊图张回...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26929</td>\n",
       "      <td>0</td>\n",
       "      <td>幸福做桌菜然家朋友吃完没帮忙刷碗老规矩昨晚想做早餐奶包发酵忘遛弯回馒头样子朋友吃午两点吃早新...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31080</td>\n",
       "      <td>0</td>\n",
       "      <td>日清明青草疫情中牺牲医护员公安干警基层干部线工作逝世胞沉痛悼念情病毒面前选择逆行选择死生选择...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20645</th>\n",
       "      <td>10321</td>\n",
       "      <td>1</td>\n",
       "      <td>意做件错事真久久原谅近发生事情绪波动房间爆哭然雨天跑出外面冻两时爸妈姨表哥电话接妈妈外面找找...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20646</th>\n",
       "      <td>10322</td>\n",
       "      <td>1</td>\n",
       "      <td>太喜欢张教授说话特实危言耸听然整体医生亲戚朋友医生医生子女微博关注医生说话客观尤专业领域戴口...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20647</th>\n",
       "      <td>10323</td>\n",
       "      <td>1</td>\n",
       "      <td>毒药断送爱听说爱情蜜糖深陷沉醉岂知份爱情美意想原理结束意想称爱情毒药爱情终止测试图点外卖点外...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20648</th>\n",
       "      <td>10324</td>\n",
       "      <td>1</td>\n",
       "      <td>果原谅痛苦值降低贼悔吃麻辣串洗完澡新换衣服全味难受新闻男熬夜猝死死女抑郁症确诊天昨天加药量睡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20649</th>\n",
       "      <td>10325</td>\n",
       "      <td>1</td>\n",
       "      <td>面兽心衣冠禽兽说求助警方包庇社真药救希正义早日坏绳法回分装爱朵手抖漏间觉浓行跑出房间五分钟进...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20650 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "0      25640      0  害真挺吃颜想岁足表达减肥决心姐妹谈恋爱感觉全世界闺蜜介绍成功行淦近新情侣装评手评寻思俩关系挺...\n",
       "1      28303      0  拥抱世界暖心动作生活中难免挫折坎坷安慌乱许拥抱充满量世界抗衡愿生山树栖心爱春赏花夏纳凉秋登山...\n",
       "2      10546      0  雨天穿滑倒坐水里亲测艾特子赟家意见麻雀妈妈问麻雀扎头发麻雀说啾啾晚子赟帮辫头发感找年糊图张回...\n",
       "3      26929      0  幸福做桌菜然家朋友吃完没帮忙刷碗老规矩昨晚想做早餐奶包发酵忘遛弯回馒头样子朋友吃午两点吃早新...\n",
       "4      31080      0  日清明青草疫情中牺牲医护员公安干警基层干部线工作逝世胞沉痛悼念情病毒面前选择逆行选择死生选择...\n",
       "...      ...    ...                                                ...\n",
       "20645  10321      1  意做件错事真久久原谅近发生事情绪波动房间爆哭然雨天跑出外面冻两时爸妈姨表哥电话接妈妈外面找找...\n",
       "20646  10322      1  太喜欢张教授说话特实危言耸听然整体医生亲戚朋友医生医生子女微博关注医生说话客观尤专业领域戴口...\n",
       "20647  10323      1  毒药断送爱听说爱情蜜糖深陷沉醉岂知份爱情美意想原理结束意想称爱情毒药爱情终止测试图点外卖点外...\n",
       "20648  10324      1  果原谅痛苦值降低贼悔吃麻辣串洗完澡新换衣服全味难受新闻男熬夜猝死死女抑郁症确诊天昨天加药量睡...\n",
       "20649  10325      1  面兽心衣冠禽兽说求助警方包庇社真药救希正义早日坏绳法回分装爱朵手抖漏间觉浓行跑出房间五分钟进...\n",
       "\n",
       "[20650 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_all_cleaned_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27693417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd74dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf55102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_0 = df2[df2['label'] == 0].sample(n=500, random_state=43)\n",
    "df_label_1 = df2[df2['label'] == 1].sample(n=500, random_state=43)\n",
    "\n",
    "# 合并两个dataframe\n",
    "df3 = pd.concat([df_label_0, df_label_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbf31430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3=df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8211fc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8426</th>\n",
       "      <td>24791</td>\n",
       "      <td>0</td>\n",
       "      <td>车王徐浪浪哥电影想红玩微拍暴力明天午两点半直播总发车仪式明天午两点半直播总发车仪式路走老脆弱...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18973</td>\n",
       "      <td>0</td>\n",
       "      <td>分享单曲喜欢原唱陈洁仪课代表估计追青刷朋友圈文案觉生畏头秃做分钟报告拍张低头发际线堪忧片放进...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10042</th>\n",
       "      <td>31405</td>\n",
       "      <td>0</td>\n",
       "      <td>魔族炼气士刚刚祭灵未扑头老龟便见道道剑气落半空中象头鹰头独眼魔神头蛛魔神头头颅雨般落选李彤拒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6144</th>\n",
       "      <td>15303</td>\n",
       "      <td>0</td>\n",
       "      <td>卡日签领红包签越红包越限时奖励快领错惜微博红包祝生日快乐湖北黄氏考察调研记浦仲诚隐梅斋湖北黄...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3002</th>\n",
       "      <td>13335</td>\n",
       "      <td>0</td>\n",
       "      <td>轉發微博轉發微博工作王道刘昊然张慧雯林姑娘腰功厉害王源确认出演玄幻电视剧主宰男主角牧尘首位担...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14730</th>\n",
       "      <td>4406</td>\n",
       "      <td>1</td>\n",
       "      <td>果样负面情绪丢真实情况想吞百颗药安心睡长长醒觉长特想回抱抱告诉洪水猛兽情绪然理解早晨点吃药睡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>忙早点床晚点半前班记前段时间站视频谈学校里抑郁杀学长宿舍楼跳血肉模糊丝意识哭说想死求求救救终...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15078</th>\n",
       "      <td>4754</td>\n",
       "      <td>1</td>\n",
       "      <td>知心理健康情况做心理测试表现出智障健忘杀意愿计划残太残年月份告张丽化名寻甸县疾病预防控制中心...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11063</th>\n",
       "      <td>739</td>\n",
       "      <td>1</td>\n",
       "      <td>胃酸流星难受嗝屁谜叔十点送入昏迷然三点醒痛苦流星夜做事聊便开播差点谜叔送昏迷流星夜切实感岛噪...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>3130</td>\n",
       "      <td>1</td>\n",
       "      <td>出贝玲妃反孔精英粉红底霜正品购买记录买两掉真妈想骂草泥马傻逼运生真艰难努力做优秀果没出色希力...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "8426   24791      0  车王徐浪浪哥电影想红玩微拍暴力明天午两点半直播总发车仪式明天午两点半直播总发车仪式路走老脆弱...\n",
       "10     18973      0  分享单曲喜欢原唱陈洁仪课代表估计追青刷朋友圈文案觉生畏头秃做分钟报告拍张低头发际线堪忧片放进...\n",
       "10042  31405      0  魔族炼气士刚刚祭灵未扑头老龟便见道道剑气落半空中象头鹰头独眼魔神头蛛魔神头头颅雨般落选李彤拒...\n",
       "6144   15303      0  卡日签领红包签越红包越限时奖励快领错惜微博红包祝生日快乐湖北黄氏考察调研记浦仲诚隐梅斋湖北黄...\n",
       "3002   13335      0  轉發微博轉發微博工作王道刘昊然张慧雯林姑娘腰功厉害王源确认出演玄幻电视剧主宰男主角牧尘首位担...\n",
       "...      ...    ...                                                ...\n",
       "14730   4406      1  果样负面情绪丢真实情况想吞百颗药安心睡长长醒觉长特想回抱抱告诉洪水猛兽情绪然理解早晨点吃药睡...\n",
       "19578   9254      1  忙早点床晚点半前班记前段时间站视频谈学校里抑郁杀学长宿舍楼跳血肉模糊丝意识哭说想死求求救救终...\n",
       "15078   4754      1  知心理健康情况做心理测试表现出智障健忘杀意愿计划残太残年月份告张丽化名寻甸县疾病预防控制中心...\n",
       "11063    739      1  胃酸流星难受嗝屁谜叔十点送入昏迷然三点醒痛苦流星夜做事聊便开播差点谜叔送昏迷流星夜切实感岛噪...\n",
       "13454   3130      1  出贝玲妃反孔精英粉红底霜正品购买记录买两掉真妈想骂草泥马傻逼运生真艰难努力做优秀果没出色希力...\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09efa08",
   "metadata": {},
   "source": [
    "# 新训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15340a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 22s 970ms/step - loss: 0.6882 - accuracy: 0.5414 - val_loss: 0.6626 - val_accuracy: 0.7600\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 21s 961ms/step - loss: 0.5755 - accuracy: 0.8929 - val_loss: 0.5901 - val_accuracy: 0.8600\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 21s 965ms/step - loss: 0.4331 - accuracy: 0.9714 - val_loss: 0.4282 - val_accuracy: 0.9467\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 22s 994ms/step - loss: 0.2510 - accuracy: 0.9757 - val_loss: 0.2567 - val_accuracy: 0.9500\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 22s 1s/step - loss: 0.1297 - accuracy: 0.9800 - val_loss: 0.1886 - val_accuracy: 0.9433\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 22s 1s/step - loss: 0.0855 - accuracy: 0.9857 - val_loss: 0.1675 - val_accuracy: 0.9433\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 21s 971ms/step - loss: 0.0651 - accuracy: 0.9914 - val_loss: 0.1599 - val_accuracy: 0.9500\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 18s 829ms/step - loss: 0.0396 - accuracy: 0.9986 - val_loss: 0.1530 - val_accuracy: 0.9533\n",
      "Epoch 9/10\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Define the model architecture\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = tf.keras.layers.Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n",
    "    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "concatenated = tf.keras.layers.Concatenate()(conv_layers)\n",
    "dropout = tf.keras.layers.Dropout(0.5)(concatenated)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c477a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test200\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test200\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "tf.saved_model.save(model, 'TextCNN_0807test200')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1116cb",
   "metadata": {},
   "source": [
    "# f20230807 解决save model问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.637 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "989d8892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 59s 2s/step - loss: 0.6636 - accuracy: 0.6450 - val_loss: 0.5937 - val_accuracy: 0.9139\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.4295 - accuracy: 0.9175 - val_loss: 0.2755 - val_accuracy: 0.9211\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 44s 1s/step - loss: 0.1680 - accuracy: 0.9408 - val_loss: 0.1688 - val_accuracy: 0.9311\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 43s 1s/step - loss: 0.1102 - accuracy: 0.9525 - val_loss: 0.1502 - val_accuracy: 0.9368\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 54s 1s/step - loss: 0.0761 - accuracy: 0.9742 - val_loss: 0.1466 - val_accuracy: 0.9400\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 62s 2s/step - loss: 0.0502 - accuracy: 0.9883 - val_loss: 0.1376 - val_accuracy: 0.9418\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 58s 2s/step - loss: 0.0374 - accuracy: 0.9917 - val_loss: 0.1329 - val_accuracy: 0.9432\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 57s 2s/step - loss: 0.0218 - accuracy: 0.9958 - val_loss: 0.1369 - val_accuracy: 0.9414\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 58s 2s/step - loss: 0.0141 - accuracy: 0.9983 - val_loss: 0.1388 - val_accuracy: 0.9418\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 56s 1s/step - loss: 0.0117 - accuracy: 0.9975 - val_loss: 0.1358 - val_accuracy: 0.9446\n",
      "88/88 [==============================] - 21s 233ms/step\n",
      "Accuracy: 0.9446428571428571\n",
      "F1 Score: 0.9435748088824172\n",
      "Recall: 0.9303661162957645\n",
      "Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "embedding_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "inputs = tf.keras.Input(shape=(max_sequence_length,))\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(inputs)\n",
    "conv_layers = []\n",
    "for filter_size in filter_sizes:\n",
    "    conv_layer = tf.keras.layers.Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)\n",
    "    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
    "    conv_layers.append(pool_layer)\n",
    "concatenated = tf.keras.layers.Concatenate()(conv_layers)\n",
    "dropout = tf.keras.layers.Dropout(0.5)(concatenated)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "history = model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a09c8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test2003\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: TextCNN_0807test2003\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model with explicit tracing\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=(None, max_sequence_length), dtype=tf.int32)])\n",
    "def predict_fn(x):\n",
    "    return model(x)\n",
    "\n",
    "tf.saved_model.save(model, 'TextCNN_0807test2003', signatures={'serving_default': predict_fn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8b36f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model using model.save()\n",
    "model.save('TextCNN_0807test2007.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c6a153b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 19s 212ms/step\n",
      "Loaded Model Accuracy: 0.9446428571428571\n",
      "Loaded Model F1 Score: 0.9435748088824172\n",
      "Loaded Model Recall: 0.9303661162957645\n",
      "Loaded Model Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('TextCNN_0807test2007.h5')\n",
    "\n",
    "# Evaluate the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_padded)\n",
    "y_pred_binary_loaded = (y_pred_loaded > 0.5).astype(int)\n",
    "\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\n",
    "f1_loaded = f1_score(y_test, y_pred_binary_loaded)\n",
    "recall_loaded = recall_score(y_test, y_pred_binary_loaded)\n",
    "precision_loaded = precision_score(y_test, y_pred_binary_loaded)\n",
    "\n",
    "print(\"Loaded Model Accuracy:\", accuracy_loaded)\n",
    "print(\"Loaded Model F1 Score:\", f1_loaded)\n",
    "print(\"Loaded Model Recall:\", recall_loaded)\n",
    "print(\"Loaded Model Precision:\", precision_loaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782a811",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb9dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c11cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b154173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')\n",
    "df2=balanced_df_all_cleaned_tokenization.copy()\n",
    "df_label_0 = df2[df2['label'] == 0].sample(n=2000, random_state=42)\n",
    "df_label_1 = df2[df2['label'] == 1].sample(n=2000, random_state=42)\n",
    "\n",
    "# 合并两个dataframe\n",
    "df3 = pd.concat([df_label_0, df_label_1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24e8f977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10148</th>\n",
       "      <td>31839</td>\n",
       "      <td>0</td>\n",
       "      <td>手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>22295</td>\n",
       "      <td>0</td>\n",
       "      <td>北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>17168</td>\n",
       "      <td>0</td>\n",
       "      <td>早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7703</th>\n",
       "      <td>21714</td>\n",
       "      <td>0</td>\n",
       "      <td>式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>29150</td>\n",
       "      <td>0</td>\n",
       "      <td>做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11487</th>\n",
       "      <td>1163</td>\n",
       "      <td>1</td>\n",
       "      <td>真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14887</th>\n",
       "      <td>4563</td>\n",
       "      <td>1</td>\n",
       "      <td>清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>1672</td>\n",
       "      <td>1</td>\n",
       "      <td>生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14184</th>\n",
       "      <td>3860</td>\n",
       "      <td>1</td>\n",
       "      <td>花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18474</th>\n",
       "      <td>8150</td>\n",
       "      <td>1</td>\n",
       "      <td>习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "10148  31839      0  手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...\n",
       "1692   22295      0  北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...\n",
       "2692   17168      0  早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...\n",
       "7703   21714      0  式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...\n",
       "321    29150      0  做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...\n",
       "...      ...    ...                                                ...\n",
       "11487   1163      1  真 难受 关注 转发 抽 粉丝 送 国风 衣裙 条条 爱爱 裙子 胸口 红花 刺绣 吸睛 裙...\n",
       "14887   4563      1  清明 长假 三天 家 两天 出浪 憋 久 天气 错 穿 汉服 浪 两天 家 睡睡 睡睡 吃饭...\n",
       "11996   1672      1  生理 趋势 杀 害怕 双相 情感 障碍 事 折磨 吃饭 哭泣 闭 抑郁 傻瓜 做事 然偶 傻...\n",
       "14184   3860      1  花邑 永远 神 加劲 克制 抑郁 失位 亲 爷爷 年位 姑年 忌日 四天 前 三月初 三 三...\n",
       "18474   8150      1  习惯 晚睡 早太 痛苦 抑郁症 患 父母 孩子 疾病 折磨 心 痛苦 抑郁症 治愈 希爱生 ...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b9c4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X = df3['tweet_content']\n",
    "y = df3['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)#0.2 for test, 0.3 for whole\n",
    "\n",
    "# Tokenize the sequences using Jieba tokenizer\n",
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb704b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any float values in X_train and X_test to strings\n",
    "X_train = [str(x) for x in X_train]\n",
    "X_test = [str(x) for x in X_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1216e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.775 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "X_train_sequences = [tokenize(x) for x in X_train]\n",
    "X_test_sequences = [tokenize(x) for x in X_test]\n",
    "\n",
    "# Convert tokens to integer sequences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train_sequences)\n",
    "\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)\n",
    "\n",
    "# Pad sequences\n",
    "max_sequence_length = 7000  # Set the maximum sequence length for padding\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2e2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 20s 219ms/step\n",
      "Loaded Model Accuracy: 0.9446428571428571\n",
      "Loaded Model F1 Score: 0.9435748088824172\n",
      "Loaded Model Recall: 0.9303661162957645\n",
      "Loaded Model Precision: 0.9571639586410635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the model\n",
    "loaded_model = tf.keras.models.load_model('TextCNN_0807test2007.h5')\n",
    "\n",
    "# Evaluate the loaded model\n",
    "y_pred_loaded = loaded_model.predict(X_test_padded)\n",
    "y_pred_binary_loaded = (y_pred_loaded > 0.5).astype(int)\n",
    "\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)\n",
    "f1_loaded = f1_score(y_test, y_pred_binary_loaded)\n",
    "recall_loaded = recall_score(y_test, y_pred_binary_loaded)\n",
    "precision_loaded = precision_score(y_test, y_pred_binary_loaded)\n",
    "\n",
    "print(\"Loaded Model Accuracy:\", accuracy_loaded)\n",
    "print(\"Loaded Model F1 Score:\", f1_loaded)\n",
    "print(\"Loaded Model Recall:\", recall_loaded)\n",
    "print(\"Loaded Model Precision:\", precision_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c60ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 7000)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 7000, 100)    16086500    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 6998, 100)    30100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 6997, 100)    40100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 6996, 100)    50100       ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " global_max_pooling1d_3 (Global  (None, 100)         0           ['conv1d_3[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_4 (Global  (None, 100)         0           ['conv1d_4[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_5 (Global  (None, 100)         0           ['conv1d_5[0][0]']               \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 300)          0           ['global_max_pooling1d_3[0][0]', \n",
      "                                                                  'global_max_pooling1d_4[0][0]', \n",
      "                                                                  'global_max_pooling1d_5[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 300)          0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            301         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16,207,101\n",
      "Trainable params: 16,207,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7586fbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a4d18a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c27dfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load('TextCNN_0705whole')# TextCNN_whole2 TextCNN_test TextCNN_0705whole\n",
    "\n",
    "# Convert the loaded model to a Keras model\n",
    "model = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cceab32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model 0807\n",
    "# Load the model\n",
    "loaded_model = tf.saved_model.load('TextCNN_0807test2002')# TextCNN_whole2 TextCNN_test TextCNN_0705whole\n",
    "\n",
    "# Convert the loaded model to a Keras model\n",
    "model = loaded_model.signatures[\"serving_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6323981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for evaluation\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6313fc07",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute __inference_signature_wrapper_6441 as input #0(zero-based) was expected to be a int32 tensor but is a float tensor [Op:__inference_signature_wrapper_6441]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1612\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1613\u001b[1;33m           return self._call_with_structured_signature(args, kwargs,\n\u001b[0m\u001b[0;32m   1614\u001b[0m                                                       cancellation_manager)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_structured_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         self._function_spec.canonicalize_function_inputs(*args, **kwargs))\n\u001b[1;32m-> 1691\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structured_signature_check_missing_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1692\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structured_signature_check_unexpected_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_structured_signature_check_missing_args\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1709\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmissing_arguments\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1710\u001b[1;33m       raise TypeError(f\"{self._structured_signature_summary()} missing \"\n\u001b[0m\u001b[0;32m   1711\u001b[0m                       \u001b[1;34m\"required arguments: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: signature_wrapper(*, x) missing required arguments: x.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9076\\3258697868.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbatch_data_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_data_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dense'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_output\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0my_pred_binary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1602\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m     \"\"\"\n\u001b[1;32m-> 1604\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1615\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1617\u001b[1;33m             return self._call_with_flat_signature(args, kwargs,\n\u001b[0m\u001b[0;32m   1618\u001b[0m                                                   cancellation_manager)\n\u001b[0;32m   1619\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1669\u001b[0m                         \u001b[1;34mf\"#{i}(zero-based) to be a Tensor; \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m                         f\"got {type(arg).__name__} ({arg}).\")\n\u001b[1;32m-> 1671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1673\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_with_structured_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# cross-replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_unused_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     return super(_WrapperFunction, self)._call_flat(args, captured_inputs,\n\u001b[0m\u001b[0;32m    139\u001b[0m                                                     cancellation_manager)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute __inference_signature_wrapper_6441 as input #0(zero-based) was expected to be a int32 tensor but is a float tensor [Op:__inference_signature_wrapper_6441]"
     ]
    }
   ],
   "source": [
    "# Evaluate the model in batches\n",
    "y_pred_binary = []\n",
    "num_batches = int(np.ceil(len(X_test_padded) / batch_size))\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(X_test_padded))\n",
    "    batch_data = X_test_padded[start_idx:end_idx]\n",
    "    batch_data_tensor = tf.constant(batch_data, dtype=tf.float32)\n",
    "    batch_output = model(batch_data_tensor)['dense']\n",
    "    batch_pred = (batch_output > 0.5).numpy().astype(int)\n",
    "    y_pred_binary.extend(batch_pred)\n",
    "\n",
    "y_pred_binary = np.array(y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bcafb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy_TextCNN = accuracy_score(y_test, y_pred_binary)\n",
    "f1_TextCNN = f1_score(y_test, y_pred_binary)\n",
    "recall_TextCNN = recall_score(y_test, y_pred_binary)\n",
    "precision_TextCNN = precision_score(y_test, y_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_TextCNN)\n",
    "print(\"F1 Score:\", f1_TextCNN)\n",
    "print(\"Recall:\", recall_TextCNN)\n",
    "print(\"Precision:\", precision_TextCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9c436b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the new tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec01a8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: [[0]]\n"
     ]
    }
   ],
   "source": [
    "# normal content\n",
    "# new_tweet_content = ['这是 一条 新的 正常 推文']\n",
    "\n",
    "# depressed content\n",
    "new_tweet_content = ['思诺思 舒乐安定 代开 抑郁 抑郁 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']\n",
    "\n",
    "new_tweet_sequences = [tokenize(tweet) for tweet in new_tweet_content]\n",
    "new_tweet_sequences = tokenizer.texts_to_sequences(new_tweet_sequences)\n",
    "new_tweet_padded = pad_sequences(new_tweet_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Convert the padded sequences to float32\n",
    "new_tweet_padded = new_tweet_padded.astype(np.float32)\n",
    "\n",
    "# Make predictions\n",
    "output = model(tf.constant(new_tweet_padded))  # Pass the data as a constant tensor\n",
    "predictions = output['dense']  # Assuming the final layer in your model is named 'dense'\n",
    "predicted_labels = (predictions > 0.5).numpy().astype(int)  # Convert the predictions to a numpy array\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e28c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f17ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
