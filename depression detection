# Import dataset

import json
import pandas as pd
import re
import jieba 
import jieba.analyse
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RandomizedSearchCV
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from scipy.stats import pearsonr
from datetime import datetime
from imblearn.over_sampling import RandomOverSampler
import scipy.stats as stats


# show all comtents in df
# pd.set_option('display.max_colwidth', None)

# Restore to default settings
pd.reset_option('display.max_colwidth')

# import the depressed user dataset
with open('depressed.json', 'r') as file:
# Load the JSON data
    depressed = json.load(file)

# Read the JSON file into a pandas DataFrame
df_depressed = pd.read_json('depressed.json')



# import the normal user dataset
with open('normal.json', 'r') as file:
    # Load the JSON data
    normal = json.load(file)

df_normal = pd.read_json('normal.json')
# normal user， label=0, has errors, need to update
df_normal['label'] = 0

df_depressed.head(2)

df_normal.head(2)

# save to csv
# df_depressed.to_csv('df_depressed.csv', index=False, encoding='utf-8')
# df_normal.to_csv('df_normal.csv', index=False, encoding='utf-8')

# read csv
# df_normal = pd.read_csv('df_normal.csv')
df_depressed = pd.read_csv('df_depressed.csv')

# Insert ID

Because there isn’t an ID column in the dataset, insert an ID for every user to identify records for future data process.

df_depressed = df_depressed.reset_index().rename(columns={'index': 'ID'})
df_depressed['ID'] += 1

df_normal = df_normal.reset_index().rename(columns={'index': 'ID'})
df_normal['ID'] += 1

df_depressed.head(2)

# Filter the DataFrame based on the 'nickname' column
# filtered_df = df_depressed[df_depressed['nickname'] == '迷失路径']

# Display the complete contents of the 'tweets' column for the filtered DataFrame
# for tweet in filtered_df['tweets']:
#    print(tweet)

df_normal.head(2)

# Data balance

原始数据集中,男:女=7924:24646=1:3.2, 男女比例失衡。如果按照一般方法做数据平衡也会存在性别失衡的问题。Label 0: label 1= 22245: 10325= 2.2:1.  Label 为1的数据比label为0的数据要多了一半左右，并且在label为0的数据中，gender为女的数据是gender为男的数据的两倍。因此，在做数据平衡时，label为1的数据不动，label 为0且gender为男的数据也不动，在label 为0 但是gender为女的数据集中，随机取5000（7726+2599-5325=5000）行，构成新的数据集，此方法可以在保证数据平衡的条件下，减少男女比例差距，将男：女=1:3.2, 缩小至男：女=1:1.6。
原始数据集中，label和gender分组结果如图所示：男:女=7924:24646=1:3.2
一般方法做数据平衡后，label和gender分组如图所示：男:女= 5041:15609= 1：3.1，与原始数据集一样，同样存在性别失衡的情况。
按照新方法做数据平衡后，label和gender分组如图所示：男：女= 7924:12726= 1:1.6，将比例为1:3降低至1:1.6可减少男女比例之间的差距。


# data balance method2

df_merged1 = pd.read_csv('df_normal users and depressed users all columns.csv')

# before balance1
grouped_label = df_merged1.groupby(['label', ]).size().reset_index(name='count')

grouped_label

grouped_gender = df_merged1.groupby(['gender', ]).size().reset_index(name='count')

grouped_gender


# Select random 5000 rows where label = 0 and gender = "女"

df_female = df_merged1[(df_merged1['label'] == 0) & (df_merged1['gender'] == '女')].sample(n=5000, random_state=42)

# Select all rows where label = 0 and gender = "男"
df_male = df_merged1[(df_merged1['label'] == 0) & (df_merged1['gender'] == '男')]

# Select all rows where label = 1
df_label_1 = df_merged1[df_merged1['label'] == 1]

# Merge all three parts of rows
df_final_balanced_all = pd.concat([df_female, df_male, df_label_1])


df_final_balanced_all

# Save final data to CSV file
df_final_balanced_all.to_csv('df_merged_balanced_all_columns.csv', index=False)

# data balance method2

df_final_balanced_all = pd.read_csv('df_merged_balanced_all_columns.csv')

# after data balance1, label
grouped_label = df_final_balanced_all.groupby(['label', ]).size().reset_index(name='count')

grouped_label

# after data balance1,gender
grouped_gender = df_final_balanced_all.groupby(['gender', ]).size().reset_index(name='count')

grouped_gender

# Data selection

select ID, label and tweets

Due to there are 18 variables in the dataset and in tweets column, there are seven variables, which is not conductive to applying the model to do text classification. Extract ID, label, and tweets columns and generate to a new data frame. Save the new data frame to csv file for future use.

df_final_balanced_all= pd.read_csv('df_merged_balanced_all_columns.csv')

df_final_balanced_all2=df_final_balanced_all.copy()

df_final_balanced_all2

df_final_balanced_all_tweets

df_final_balanced_all_tweets = df_final_balanced_all2[['ID', 'label']].copy()

# Convert string representation of dictionaries to actual dictionaries
import ast
df_final_balanced_all2['tweets'] = df_final_balanced_all2['tweets'].apply(ast.literal_eval)

df_final_balanced_all_tweets['tweet_content'] = df_final_balanced_all2['tweets'].apply(lambda x: [d['tweet_content'] for d in x])


# after data balance1
grouped_label = df_final_balanced_all_tweets.groupby(['label', ]).size().reset_index(name='count')

grouped_label

# Save final data to CSV file
df_final_balanced_all_tweets.to_csv('df_final_balanced_ID_label_tweets.csv', index=False)

# Data clean

balanced_df_all = pd.read_csv('df_final_balanced_ID_label_tweets.csv')

### 15649
balanced_df_all.loc[balanced_df_all['ID'] == 15649]

# Load stopwords
with open('merged_stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = f.read().splitlines()

def clean_text(text):
   
    # Remove numbers, emojis, punctuations,special charts,shared posts, hashtags
    # Only Chinese characters are reserved.
    text = re.sub(r'[^\u4e00-\u9fff]+', '', text)
    
    # Remove weibo stop words
    for word in stopwords:
        if word in text:
            text = re.sub(word, '', text)   
    return text

balanced_df_all_cleaned = balanced_df_all.copy()

balanced_df_all_cleaned.loc[:, 'tweet_content'] = balanced_df_all_cleaned['tweet_content'].apply(clean_text)

balanced_df_all_cleaned

# save to csv
balanced_df_all_cleaned.to_csv('balanced_df_all_cleaned.csv', index=False, encoding='utf-8')

# Tokenization

## data cleaning>tokenization

balanced_df_all_cleaned = pd.read_csv('balanced_df_all_cleaned.csv')

balanced_df_all_cleaned.loc[balanced_df_all_cleaned['ID'] == 15649]

balanced_df_all_cleaned

# dftest=balanced_df_all_cleaned_tokenization[(balanced_df_all_cleaned_tokenization['ID'] == 30838) | (balanced_df_all_cleaned_tokenization['ID'] == 24223)]

balanced_df_all_cleaned2 = balanced_df_all_cleaned.copy()

balanced_df_all_cleaned2

balanced_df_all_cleaned2['tweet_content'] = balanced_df_all_cleaned2['tweet_content'].astype(str)

balanced_df_all_cleaned2['tweet_content'] = balanced_df_all_cleaned2['tweet_content'].apply(lambda x: ' '.join(jieba.cut(x)))

balanced_df_all_cleaned2

# save to csv
balanced_df_all_cleaned2.to_csv('balanced_df_all_cleaned_tokenization.csv', index=False, encoding='utf-8')

# Feature extraction (Start here)

TF-IDF

balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')

balanced_df_all_cleaned_tokenization.loc[balanced_df_all_cleaned_tokenization['ID'] == 15649]

balanced_df_all_cleaned_tokenization

balanced_df_all_cleaned_tokenization2 = balanced_df_all_cleaned_tokenization.copy()

# replace NaN values with an empty string
balanced_df_all_cleaned_tokenization2['tweet_content'] = balanced_df_all_cleaned_tokenization2['tweet_content'].fillna('')

# create a TfidfVectorizer object 将tweet_content列作为特征向量化
vectorizer = TfidfVectorizer()

vectorizer

# fit and transform the data
X = vectorizer.fit_transform(balanced_df_all_cleaned_tokenization2['tweet_content'])

X

# get the feature names, # 得到语料库所有不重复的词
# print(vectorizer.get_feature_names_out() )

# 将label列作为目标变量
y = balanced_df_all_cleaned_tokenization2['label']

save to txt

with open('feature_names.txt', 'w') as f:
    for item in vectorizer.get_feature_names_out():
        f.write("%s\n" % item)


# models

## Split the dataset into training and test sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

## Random Forest

# 训练随机森林模型
rfc = RandomForestClassifier(n_estimators=13) # 修改树的棵树， 使用10折交叉验证来确定n_estimators的最佳值为13
rfc.fit(X_train, y_train)

# 预测测试集的label
y_pred_rfc = rfc.predict(X_test)

# 计算模型评估指标
accuracy_rf = accuracy_score(y_test, y_pred_rfc)
precision_rf = precision_score(y_test, y_pred_rfc)
recall_rf = recall_score(y_test, y_pred_rfc)
f1_rf = f1_score(y_test, y_pred_rfc)

print('Accuracy:', accuracy_rf)
print('Precision:', precision_rf)
print('Recall:', recall_rf)
print('F1 Score:', f1_rf)

# normal content
new_tweet_content = ['这是 一条 新的 正常 推文']

# depressed content
# new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']

# feature extraction
new_X = vectorizer.transform(new_tweet_content)

# RF
new_y_pred_rfc = rfc.predict(new_X)

print('\nnew_tweet_content label:')
print('RF:', new_y_pred_rfc)

## SVM

# 训练SVM模型
svc = SVC(kernel='linear')
svc.fit(X_train, y_train)

# 预测测试集的label
y_pred_svc = svc.predict(X_test)







# 计算SVM模型评估指标
accuracy_svc = accuracy_score(y_test, y_pred_svc)
precision_svc = precision_score(y_test, y_pred_svc)
recall_svc = recall_score(y_test, y_pred_svc)
f1_svc = f1_score(y_test, y_pred_svc)

print('\nSupport Vector Machine:')
print('Accuracy:', accuracy_svc)
print('Precision:', precision_svc)
print('Recall:', recall_svc)
print('F1 Score:', f1_svc)

# normal content
# new_tweet_content = ['这是 一条 新的 正常 推文']

# depressed content
new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']

# feature extraction
new_X = vectorizer.transform(new_tweet_content)

# SVM
new_y_pred_svc = svc.predict(new_X)

print('\nnew_tweet_content label:')
print('SVM:', new_y_pred_svc)

# normal content
# new_tweet_content = ['这是 一条 新的 正常 推文']

# depressed content
new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']

# feature extraction
new_X = vectorizer.transform(new_tweet_content)

# SVM
new_y_pred_svc = svc.predict(new_X)
new_y_pred_rfc = rfc.predict(new_X)

print('\nnew_tweet_content label:')
print('SVM:', new_y_pred_svc)
print('RF:', new_y_pred_rfc)

## MLP

mlp = MLPClassifier(hidden_layer_sizes=(30,), max_iter=20,batch_size=128)

adam： Kingma, Diederik, and Jimmy Ba提出的机遇随机梯度的优化器 注意：默认solver ‘adam’在相对较大的数据集上效果比较好.
early_stopping : bool, default False,Only effective when solver=’sgd’ or ‘adam’,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续爹迭代改善低于tol时终止训练.batchsize并非越大越好，一个经验性的可选数值是32、64、128，一般来说，每个batch的大小一旦超过64，继续增大batch带来的性能收益就微乎其微了

mlp.fit(X_train, y_train)

我使用RandomizedSearchCV搜索最佳超参数组合。GridSearchCV和RandomizedSearchCV都是用于超参数调优的方法，但它们的实现方式不同。GridSearchCV会遍历所有可能的参数组合，而RandomizedSearchCV则会随机选择一些参数组合进行训练。当数据集较小时，我们可以使用GridSearchCV，但当数据集较大时，我们可以使用RandomizedSearchCV

Best parameters: {'solver': 'adam', 'max_iter': 1500, 'hidden_layer_sizes': (100,), 'activation': 'relu'}
Best cross-validation score: 0.90
用100条数据进行测试， 50条0,50条1

# 预测测试集的label
y_pred_mlp = mlp.predict(X_test)

# 计算MLP模型评估指标
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)

print('\nMulti-layer Perceptron:')
print('Accuracy:', accuracy_mlp)
print('Precision:', precision_mlp)
print('Recall:', recall_mlp)
print('F1 Score:', f1_mlp)


# 将新的tweet_content转换为特征向量
# new_tweet_content = ['这是 一条 新的 正常 推文']

# depressed content
new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']


# feature extraction
new_X = vectorizer.transform(new_tweet_content)

# 使用MLP模型进行预测
new_y_pred_mlp = mlp.predict(new_X)

print('MLP模型:', new_y_pred_mlp)

## TextCNN

# Load the dataset
balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')

balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')

df3=balanced_df_all_cleaned_tokenization.copy()

df3.head()

# Preprocess the data
X = df3['tweet_content']
y = df3['label']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Tokenize the sequences using Jieba tokenizer
def tokenize(text):
    return list(jieba.cut(text))
X_train_sequences = [' '.join(tokenize(x)) for x in X_train]
X_test_sequences = [' '.join(tokenize(x)) for x in X_test]


# Convert tokens to integer sequences
tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(X_train_sequences)

# Convert tokenized sequences to integer sequences
X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)
X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)

# Pad sequences
max_sequence_length = 7000  # Set the maximum sequence length for padding
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')


print(X_test_padded.shape)

# Define the model architecture
embedding_dim = 100
num_filters = 100
filter_sizes = [3, 4, 5]

inputs = tf.keras.Input(shape=(max_sequence_length,))
embedding_layer = tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim)(inputs)
conv_layers = []
for filter_size in filter_sizes:
    conv_layer = tf.keras.layers.Conv1D(num_filters, filter_size, activation='relu')(embedding_layer)
    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)
    conv_layers.append(pool_layer)
concatenated = tf.keras.layers.Concatenate()(conv_layers)
dropout = tf.keras.layers.Dropout(0.5)(concatenated)
outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
batch_size = 32
epochs = 10
history = model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))

# Evaluate the model
y_pred = model.predict(X_test_padded)
y_pred_binary = (y_pred > 0.5).astype(int)

accuracy = accuracy_score(y_test, y_pred_binary)
f1 = f1_score(y_test, y_pred_binary)
recall = recall_score(y_test, y_pred_binary)
precision = precision_score(y_test, y_pred_binary)

print("Accuracy:", accuracy)
print("F1 Score:", f1)
print("Recall:", recall)
print("Precision:", precision)



## Save models

# import pickle

### SVM

# 保存模型
with open('svm_whole2.pkl', 'wb') as file:
    pickle.dump(svc, file)


# 模型读取
with open('svm_whole2.pkl', 'rb') as file:
    svc = pickle.load(file)

### MLP

# 保存模型
with open('mlp_whole2.pkl', 'wb') as file:
    pickle.dump(mlp, file)

# 模型读取
with open('mlp_whole2.pkl', 'rb') as file:
    mlp = pickle.load(file)

### RF

# 保存模型
with open('rf_whole2.pkl', 'wb') as file:
    pickle.dump(rfc, file)

# 模型读取
with open('rf_whole2.pkl', 'rb') as file:
    rfc = pickle.load(file)

### TextCNN

# Save the model using model.save() # use this method
model.save('TextCNN_0807whole.h5')



# Load the model
loaded_model = tf.keras.models.load_model('TextCNN_0807test2007.h5')




# Save the model
tf.saved_model.save(model, 'TextCNN_whole2')

# Load the model
loaded_model = tf.saved_model.load('TextCNN_whole2')

# load and use models

## RF, SVM, MLP

### 模型使用 after feature extraction and split the dataset, start here, no textCNN

balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')
balanced_df_all_cleaned_tokenization2 = balanced_df_all_cleaned_tokenization.copy()
# replace NaN values with an empty string
balanced_df_all_cleaned_tokenization2['tweet_content'] = balanced_df_all_cleaned_tokenization2['tweet_content'].fillna('')
# create a TfidfVectorizer object 将tweet_content列作为特征向量化
vectorizer = TfidfVectorizer()
# fit and transform the data
X = vectorizer.fit_transform(balanced_df_all_cleaned_tokenization2['tweet_content'])
# 将label列作为目标变量
y = balanced_df_all_cleaned_tokenization2['label']
# split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

#rf
with open('rf_whole2.pkl', 'rb') as file:
    rfc = pickle.load(file)

# svm
with open('svm_whole2.pkl', 'rb') as file:
    svc = pickle.load(file)

# mlp    
with open('mlp_whole2.pkl', 'rb') as file:
    mlp = pickle.load(file)

# Evaluation
y_pred_rfc = rfc.predict(X_test)
y_pred_svc = svc.predict(X_test)
y_pred_mlp = mlp.predict(X_test)

rfc_accuracy = accuracy_score(y_test, y_pred_rfc)
rfc_precision = precision_score(y_test, y_pred_rfc)
rfc_recall = recall_score(y_test, y_pred_rfc)
rfc_f1 = f1_score(y_test, y_pred_rfc)

accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)

accuracy_svc = accuracy_score(y_test, y_pred_svc)
precision_svc = precision_score(y_test, y_pred_svc)
recall_svc = recall_score(y_test, y_pred_svc)
f1_svc = f1_score(y_test, y_pred_svc)



print('\nRandom Forest:')
print('Accuracy:', rfc_accuracy)
print('Precision:', rfc_precision)
print('Recall:', rfc_recall)
print('F1 Score:', rfc_f1)

print('\nMulti-layer Perceptron:')
print('Accuracy:', accuracy_mlp)
print('Precision:', precision_mlp)
print('Recall:', recall_mlp)
print('F1 Score:', f1_mlp)

print('\nSupport Vector Machine:')
print('Accuracy:', accuracy_svc)
print('Precision:', precision_svc)
print('Recall:', recall_svc)
print('F1 Score:', f1_svc)

# results, don't run following code, that's trained model's reslunts, not loaded model's results

import pandas as pd

# Create a dictionary with the evaluation results(training model results, not loaded model's results)
results = {
    'Model': ['Random Forest', 'Multi-layer Perceptron', 'Support Vector Machine'],
    'Accuracy': [accuracy, accuracy_mlp, accuracy_svc],
    'Precision': [precision, precision_mlp, precision_svc],
    'Recall': [recall, recall_mlp, recall_svc],
    'F1 Score': [f1, f1_mlp, f1_svc]
}

# Convert the dictionary to a dataframe
df_results = pd.DataFrame(results)

# Display the dataframe
print(df_results)


# add textCNN evaluation


# Create a dictionary with the evaluation results for TextCNN
textcnn_results = {
    'Model': ['TextCNN'],
    'Accuracy': [accuracy_TextCNN],
    'Precision': [precision_TextCNN],
    'Recall': [recall_TextCNN],
    'F1 Score': [f1_TextCNN]
}

# Add the TextCNN results to the existing results dictionary
results['Model'].append('TextCNN')
results['Accuracy'].append(accuracy_TextCNN)
results['Precision'].append(precision_TextCNN)
results['Recall'].append(recall_TextCNN)
results['F1 Score'].append(f1_TextCNN)

# Convert the dictionary to a dataframe
df_results = pd.DataFrame(results)

# Display the dataframe
print(df_results)


# save to csv
df_results.to_csv('models_results.csv', index=False, encoding='utf-8')

df_results

# 将新的tweet_content转换为特征向量
# new_tweet_content = ['这是 一条 新的 正常 推文']

# depressed content
new_tweet_content = ['思 诺思 舒乐安定 代开 疫情 期间 开药 困难 开药 断药 关注 南京 兼职 超话志道 合盆友 解释 快 超话 传送门 南京 兼职']


# 使用随机森林模型进行预测
new_y_pred_rfc = rfc.predict(new_X)

# 使用SVM模型进行预测
new_y_pred_svc = svc.predict(new_X)

# 使用MLP模型进行预测
new_y_pred_mlp = mlp.predict(new_X)

print('\n新的tweet_content的预测标签:')
print('随机森林模型:', new_y_pred_rfc)
print('SVM模型:', new_y_pred_svc)
print('MLP模型:', new_y_pred_mlp)

## TextCNN

# Load the dataset
balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')

balanced_df_all_cleaned_tokenization['tweet_content'] = balanced_df_all_cleaned_tokenization['tweet_content'].fillna('')

df3=balanced_df_all_cleaned_tokenization.copy()

# Preprocess the data
X = df3['tweet_content']
y = df3['label']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # test, 

# Tokenize the sequences using Jieba tokenizer
def tokenize(text):
    return list(jieba.cut(text))
X_train_sequences = [' '.join(tokenize(x)) for x in X_train]
X_test_sequences = [' '.join(tokenize(x)) for x in X_test]


# Convert tokens to integer sequences
tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(X_train_sequences)


# Convert tokenized sequences to integer sequences
X_train_sequences = tokenizer.texts_to_sequences(X_train_sequences)
X_test_sequences = tokenizer.texts_to_sequences(X_test_sequences)


# Pad sequences
max_sequence_length = 7000  # Set the maximum sequence length for padding
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post')


print(X_test_padded.shape)

# Load the model
loaded_model = tf.keras.models.load_model('TextCNN_0807whole.h5')

# Evaluate the loaded model
y_pred_loaded = loaded_model.predict(X_test_padded)
y_pred_binary_loaded = (y_pred_loaded > 0.5).astype(int)

accuracy_loaded = accuracy_score(y_test, y_pred_binary_loaded)
f1_loaded = f1_score(y_test, y_pred_binary_loaded)
recall_loaded = recall_score(y_test, y_pred_binary_loaded)
precision_loaded = precision_score(y_test, y_pred_binary_loaded)

print("Loaded Model Accuracy:", accuracy_loaded)
print("Loaded Model F1 Score:", f1_loaded)
print("Loaded Model Recall:", recall_loaded)
print("Loaded Model Precision:", precision_loaded)


after load all models, run the below code

print('\nRandom Forest:')
print('Accuracy:', rfc_accuracy)
print('Precision:', rfc_precision)
print('Recall:', rfc_recall)
print('F1 Score:', rfc_f1)

print('\nMulti-layer Perceptron:')
print('Accuracy:', accuracy_mlp)
print('Precision:', precision_mlp)
print('Recall:', recall_mlp)
print('F1 Score:', f1_mlp)

print('\nSupport Vector Machine:')
print('Accuracy:', accuracy_svc)
print('Precision:', precision_svc)
print('Recall:', recall_svc)
print('F1 Score:', f1_svc)

print('\nTest CNN:')
print("Accuracy:", accuracy_TextCNN)
print("F1 Score:", f1_TextCNN)
print("Recall:", recall_TextCNN)
print("Precision:", precision_TextCNN)





# Feature identification for users' profile

df_merged_balanced_all_columns= pd.read_csv('df_merged_balanced_all_columns.csv')

df_merged=df_merged_balanced_all_columns.copy()

# variables to describe the user's profile
df_user=df_merged[['ID','label','gender','profile','birthday','num_of_follower','num_of_following','all_tweet_count',
                                    'original_tweet_count','repost_tweet_count']]

df_user

# Filter the data for label 1 and label 0
label_1_data = df_user[df_user['label'] == 1]
label_0_data = df_user[df_user['label'] == 0]

label_1_data

label_0_data

#convert the birthday column to datetime format
df_user['birthday'] = pd.to_datetime(df_user['birthday'], format='%Y-%m-%d', errors='coerce')


#calculate the age of each person and add it as a new column
df_user['age'] = (datetime.now() - df_user['birthday']).astype('<m8[Y]')

df_user['gender'] = df_user['gender'].replace({'男': 'Male', '女': 'Female', '无': pd.NaT})

#show gender count
gender = df_user['gender'].value_counts()
print(gender)

## gender

df_user['gender'] = df_user['gender'].replace({'男': 'Male', '女': 'Female', '无': pd.NaT})

# show gender count
gender = df_user['gender'].value_counts()
print(gender)

# group the dataframe by label and gender
grouped = df_user.groupby(['label', 'gender']).size().reset_index(name='count')

grouped

# pivot the dataframe to create a table of counts
table = grouped.pivot(index='gender', columns='label', values='count')
print(table)

ax = table.plot(kind='bar')


# Display the numbers on the plot
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom')

plt.xlabel("gender")
plt.ylabel("count")
plt.title("Gender distribution in the dataset")
plt.show()

# Grouped data
labels = ['Female, Label 0',  'Male, Label 0', 'Female, Label 1','Male, Label 1']
sizes = grouped['count']
explode = (0, 0, 0, 0)  # No slice will be exploded

# Plotting the pie chart
plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=90)

# Add a title
plt.title('Percentage Distribution by Gender and Label in the dataset')

# Equal aspect ratio ensures that pie is drawn as a circle
plt.axis('equal')

# Display the pie chart
plt.show()

# Filter the data for males and females
male_data = grouped[grouped['gender'] == 'Male']
female_data = grouped[grouped['gender'] == 'Female']

# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(1, 2)

# Pie chart for males
labels_male = ['Label 0', 'Label 1']
sizes_male = male_data['count']
explode_male = (0, 0)  # No slice will be exploded
axes[0].pie(sizes_male, explode=explode_male, labels=labels_male, autopct='%1.1f%%', startangle=90)
axes[0].set_title('Percentage Distribution for Males')

# Pie chart for females
labels_female = ['Label 0', 'Label 1']
sizes_female = female_data['count']
explode_female = (0, 0)  # No slice will be exploded
axes[1].pie(sizes_female, explode=explode_female, labels=labels_female, autopct='%1.1f%%', startangle=90)
axes[1].set_title('Percentage Distribution for Females')

# Equal aspect ratio ensures that pies are drawn as circles
axes[0].axis('equal')
axes[1].axis('equal')

# Adjust the spacing between subplots
plt.subplots_adjust(wspace=0.6)

# Display the plot
plt.show()

## age

# convert the birthday column to datetime format
df_user['birthday'] = pd.to_datetime(df_user['birthday'], format='%Y-%m-%d', errors='coerce')


# calculate the age of each person and add it as a new column
df_user['age'] = (datetime.now() - df_user['birthday']).astype('<m8[Y]')

# Filter the data for label 1 and label 0
label_1_data = df_user[df_user['label'] == 1]
label_0_data = df_user[df_user['label'] == 0]


label_0_data

label_0_data

# Calculate the age distribution for label 1 and label 0
age_counts_label_1 = label_1_data['age'].value_counts().sort_index()
age_counts_label_0 = label_0_data['age'].value_counts().sort_index()

age_counts_label_0


# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='age',color='blue', kde=True, ax=axes[0])
axes[0].set_xlabel('Age')
axes[0].set_ylabel('Count')
axes[0].set_title('Age Distribution - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data, x='age', color='green',kde=True, ax=axes[1])
axes[1].set_xlabel('Age')
axes[1].set_ylabel('Count')
axes[1].set_title('Age Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()


## num_of_follower

label_1_data

# Calculate the age distribution for label 1 and label 0
follower_counts_label_1 = label_1_data['num_of_follower'].value_counts().sort_index()
follower_counts_label_0 = label_0_data['num_of_follower'].value_counts().sort_index()

follower_counts_label_1

follower_counts_label_0

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='num_of_follower', color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlabel('Follower')
axes[0].set_ylabel('Count')
axes[0].set_title('Follower Distribution - Label 1')

# Bar chart for label 0
sns.histplot(data=label_0_data, x='num_of_follower', color='green', kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('Follower')
axes[1].set_ylabel('Count')
axes[1].set_title('Follower Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()


# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Box plot for label 1
sns.boxplot(data=label_1_data, x='num_of_follower', color='blue', ax=axes[0])
axes[0].set_xlabel('Follower')
axes[0].set_ylabel('Count')
axes[0].set_title('Follower Distribution - Label 1')

# Box plot for label 0
sns.boxplot(data=label_0_data, x='num_of_follower', color='green', ax=axes[1])
axes[1].set_xlabel('Follower')
axes[1].set_ylabel('Count')
axes[1].set_title('Follower Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()


# Calculate the interquartile range (IQR) for each label
iqr_label_1 = label_1_data['num_of_follower'].quantile(0.75) - label_1_data['num_of_follower'].quantile(0.25)
iqr_label_0 = label_0_data['num_of_follower'].quantile(0.75) - label_0_data['num_of_follower'].quantile(0.25)

# Define the lower and upper bounds for each label
lower_bound_label_1 = label_1_data['num_of_follower'].quantile(0.25) - 1.5 * iqr_label_1
upper_bound_label_1 = label_1_data['num_of_follower'].quantile(0.75) + 1.5 * iqr_label_1
lower_bound_label_0 = label_0_data['num_of_follower'].quantile(0.25) - 1.5 * iqr_label_0
upper_bound_label_0 = label_0_data['num_of_follower'].quantile(0.75) + 1.5 * iqr_label_0

# Filter out the outliers for each label
label_1_data_no_outliers = label_1_data[(label_1_data['num_of_follower'] >= lower_bound_label_1) & (label_1_data['num_of_follower'] <= upper_bound_label_1)]
label_0_data_no_outliers = label_0_data[(label_0_data['num_of_follower'] >= lower_bound_label_0) & (label_0_data['num_of_follower'] <= upper_bound_label_0)]


# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data_no_outliers, x='num_of_follower', color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlabel('Follower')
axes[0].set_ylabel('Count')
axes[0].set_title('Follower Distribution_no_outliers - Label 1')

# Bar chart for label 0
sns.histplot(data=label_0_data_no_outliers, x='num_of_follower', color='green', kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('Follower')
axes[1].set_ylabel('Count')
axes[1].set_title('Follower Distribution_no_outliers - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()


## num_of_following

# Calculate the age distribution for label 1 and label 0
following_counts_label_1 = label_1_data['num_of_following'].value_counts().sort_index()
following_counts_label_0 = label_0_data['num_of_following'].value_counts().sort_index()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='num_of_following',color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlabel('Following')
axes[0].set_ylabel('Count')
axes[0].set_title('Following Distribution - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data, x='num_of_following', color='green',kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('Following')
axes[1].set_ylabel('Count')
axes[1].set_title('Following Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

## all_tweet_count

# Calculate the age distribution for label 1 and label 0
following_counts_label_1 = label_1_data['all_tweet_count'].value_counts().sort_index()
following_counts_label_0 = label_0_data['all_tweet_count'].value_counts().sort_index()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='all_tweet_count',color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlabel('all_tweet_count')
axes[0].set_ylabel('Count')
axes[0].set_title('all_tweet_count - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data, x='all_tweet_count', color='green',kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('all_tweet_count')
axes[1].set_ylabel('Count')
axes[1].set_title('all_tweet_count - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

# Calculate the interquartile range (IQR) for each label
iqr_label_1 = label_1_data['all_tweet_count'].quantile(0.75) - label_1_data['all_tweet_count'].quantile(0.25)
iqr_label_0 = label_0_data['all_tweet_count'].quantile(0.75) - label_0_data['all_tweet_count'].quantile(0.25)

# Define the lower and upper bounds for each label
lower_bound_label_1 = label_1_data['all_tweet_count'].quantile(0.25) - 1.5 * iqr_label_1
upper_bound_label_1 = label_1_data['all_tweet_count'].quantile(0.75) + 1.5 * iqr_label_1
lower_bound_label_0 = label_0_data['all_tweet_count'].quantile(0.25) - 1.5 * iqr_label_0
upper_bound_label_0 = label_0_data['all_tweet_count'].quantile(0.75) + 1.5 * iqr_label_0

# Filter out the outliers for each label
label_1_data_no_outliers = label_1_data[(label_1_data['all_tweet_count'] >= lower_bound_label_1) & (label_1_data['all_tweet_count'] <= upper_bound_label_1)]
label_0_data_no_outliers = label_0_data[(label_0_data['all_tweet_count'] >= lower_bound_label_0) & (label_0_data['all_tweet_count'] <= upper_bound_label_0)]


# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data_no_outliers, x='all_tweet_count',color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlabel('all_tweet_count')
axes[0].set_ylabel('Count')
axes[0].set_title('all_tweet_count_no_outliers - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data_no_outliers, x='all_tweet_count', color='green',kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('all_tweet_count')
axes[1].set_ylabel('Count')
axes[1].set_title('all_tweet_count_no_outliers - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

##  original_tweet_count

# Calculate the age distribution for label 1 and label 0
original_tweets_counts_label_1 = label_1_data['original_tweet_count'].value_counts().sort_index()
original_tweets_label_0 = label_0_data['original_tweet_count'].value_counts().sort_index()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='original_tweet_count',color='blue', kde=True, ax=axes[0],element='step')
axes[0].set_xlabel('Original_tweets')
axes[0].set_ylabel('Count')
axes[0].set_title('Original_tweets Distribution - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data, x='original_tweet_count', color='green',kde=True, ax=axes[1],element='step')
axes[1].set_xlabel('Original_tweets')
axes[1].set_ylabel('Count')
axes[1].set_title('Original_tweets Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='original_tweet_count', color='blue', kde=True, ax=axes[0], element='step')
axes[0].set_xlim(0, 150)  # Set x-axis limit to show only the range of 0-150
axes[0].set_xlabel('Original_tweets')
axes[0].set_ylabel('Count')
axes[0].set_title('Original_tweets Distribution_0 to 150 - Label 1')

# Bar chart for label 0
sns.histplot(data=label_0_data, x='original_tweet_count', color='green', kde=True, ax=axes[1], element='step')
axes[1].set_xlabel('Original_tweets')
axes[1].set_ylabel('Count')
axes[1].set_title('Original_tweets Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()


# Calculate the interquartile range (IQR) for each label
iqr_label_1 = label_1_data['original_tweet_count'].quantile(0.75) - label_1_data['original_tweet_count'].quantile(0.25)
iqr_label_0 = label_0_data['original_tweet_count'].quantile(0.75) - label_0_data['original_tweet_count'].quantile(0.25)

# Define the lower and upper bounds for each label
lower_bound_label_1 = label_1_data['original_tweet_count'].quantile(0.25) - 1.5 * iqr_label_1
upper_bound_label_1 = label_1_data['original_tweet_count'].quantile(0.75) + 1.5 * iqr_label_1
lower_bound_label_0 = label_0_data['original_tweet_count'].quantile(0.25) - 1.5 * iqr_label_0
upper_bound_label_0 = label_0_data['original_tweet_count'].quantile(0.75) + 1.5 * iqr_label_0

# Filter out the outliers for each label
label_1_data_no_outliers = label_1_data[(label_1_data['original_tweet_count'] >= lower_bound_label_1) & (label_1_data['original_tweet_count'] <= upper_bound_label_1)]
label_0_data_no_outliers = label_0_data[(label_0_data['original_tweet_count'] >= lower_bound_label_0) & (label_0_data['original_tweet_count'] <= upper_bound_label_0)]


# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data_no_outliers, x='original_tweet_count',color='blue', kde=True, ax=axes[0],element='step')
axes[0].set_xlabel('Original_tweets')
axes[0].set_ylabel('Count')
axes[0].set_title('Original_tweets Distribution_no_outliers - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data_no_outliers, x='original_tweet_count', color='green',kde=True, ax=axes[1],element='step')
axes[1].set_xlabel('Original_tweets')
axes[1].set_ylabel('Count')
axes[1].set_title('Original_tweets Distribution_no_outliers - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

## repost_tweet_count

# Calculate the age distribution for label 1 and label 0
following_counts_label_1 = label_1_data['repost_tweet_count'].value_counts().sort_index()
following_counts_label_0 = label_0_data['repost_tweet_count'].value_counts().sort_index()

# Create a figure with two subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 6))

# Bar chart for label 1
sns.histplot(data=label_1_data, x='repost_tweet_count',color='blue', kde=True, ax=axes[0],element='step')
axes[0].set_xlabel('Repost_tweets')
axes[0].set_ylabel('Count')
axes[0].set_title('Repost_tweets Distribution - Label 1')



# Bar chart for label 0
sns.histplot(data=label_0_data, x='repost_tweet_count', color='green',kde=True, ax=axes[1],element='step')
axes[1].set_xlabel('Repost_tweets')
axes[1].set_ylabel('Count')
axes[1].set_title('Repost_tweets Distribution - Label 0')

# Adjust spacing between subplots
plt.tight_layout()

# Display the plot
plt.show()

# correlation analysis

df_user

user_profile=df_user[['age','num_of_follower','num_of_following','all_tweet_count','original_tweet_count','repost_tweet_count']]

user_profile

# Calculate correlation matrix
corr_matrix = user_profile.corr()

corr_matrix

## heatmap

# plot a heatmap
f, ax = plt.subplots(1, 1, figsize=(10, 10))

mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
ax.text(2.5, -0.1, 'Correlation heatmap', fontsize=18, fontweight='bold', fontfamily='serif')
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu', 
            square=True, mask=mask, linewidth=0.7, ax=ax)
plt.show()

## point-biserial correlation coefficient. 

The point-biserial correlation coefficient is used to measure the association between a categorical variable with two categories (such as 'label') and a continuous numerical variable.

user_profile=df_user[['label','age','num_of_follower','num_of_following','all_tweet_count','original_tweet_count','repost_tweet_count']]

user_profile

# Count the number of NaN values in the age column
nan_count = user_profile['age'].isnull().sum()

# Print the result
print(f'There are {nan_count} NaN values in the age column.')

# Calculate the mean of the age column
age_mean = user_profile['age'].mean()

# Print the result
print(f'The mean of the age column is {age_mean:.2f}.')

# Fill the NaN values in the age column with the mean of the column
user_profile['age'] = user_profile['age'].fillna(user_profile['age'].mean())


# Create a list of numerical variables
num_vars = ['age','num_of_follower','num_of_following','all_tweet_count','original_tweet_count','repost_tweet_count']

# Loop through the list and calculate the point-biserial correlation coefficient for each variable with label
for var in num_vars:
  # Use the pointbiserialr function from stats module
  # The first argument is the categorical variable (label)
  # The second argument is the numerical variable (var)
  r, p = stats.pointbiserialr(user_profile['label'], user_profile[var])
  # Print the results
  print(f'The point-biserial correlation coefficient between label and {var} is {r:.3f} with a p-value of {p:.3f}')


# Create a list of numerical variables
num_vars = ['age', 'num_of_follower', 'num_of_following', 'all_tweet_count', 'original_tweet_count', 'repost_tweet_count']

# Initialize lists to store correlation coefficients and p-values
correlation_coeffs = []
p_values = []

# Calculate point-biserial correlation coefficients and p-values
for var in num_vars:
    r, p = stats.pointbiserialr(user_profile['label'], user_profile[var])
    correlation_coeffs.append(r)
    p_values.append(p)

# Create a figure and axis for the bar plot
fig, ax = plt.subplots(figsize=(10, 6))

# Create the bar plot with error bars representing confidence intervals
x_pos = np.arange(len(num_vars))
bar_width = 0.4
bars = ax.bar(x_pos, correlation_coeffs, bar_width, color='g', label='Correlation Coefficient')

# Set x-axis labels and title
ax.set_xticks(x_pos)
ax.set_xticklabels(num_vars, rotation=45, ha='right')
ax.set_xlabel('Numerical Variables')
ax.set_ylabel('Point-Biserial Correlation Coefficient')
ax.set_title('Point-Biserial Correlation Coefficients between Label and Numerical Variables')

# Add the significance level indicator
sig_level = 0.05
ax.axhline(sig_level, color='r', linestyle='--', label='Significance Level (0.05)')

# Add the legend
ax.legend()

# Add correlation coefficient numbers and p-values above each bar
for i, bar in enumerate(bars):
    height = bar.get_height()
    ax.text(bar.get_x() + bar.get_width() / 2, height, f'r={correlation_coeffs[i]:.3f}', ha='center', va='bottom', fontsize=10)
    p_val = p_values[i]
    if p_val <= sig_level:
        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.05, f'p={p_val:.3f}*', ha='center', va='bottom', fontsize=10, color='blue')
    else:
        ax.text(bar.get_x() + bar.get_width() / 2, height + 0.05, f'p={p_val:.3f}', ha='center', va='bottom', fontsize=10)

# Display the plot
plt.tight_layout()
plt.show()

# Word cloud

balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')

df = balanced_df_all_cleaned_tokenization.copy()

df

# Create a word cloud for label 0
text = ' '.join(map(str, df[df['label'] == 0]['tweet_content']))
# text = ' '.join(df[df['label'] == 0]['tweet_content'])
font_path = r'C:\Windows\Fonts\simfang.ttf'#设置中文字体，否则词云图可能不显示中文
wordcloud = WordCloud(width=800, height=800, background_color='white',font_path=font_path).generate(text)

plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("Word cloud of normal users' tweets")

# Create a word cloud for label 1
text = ' '.join(map(str, df[df['label'] == 1]['tweet_content']))
# text = ' '.join(df[df['label'] == 1]['tweet_content'])
wordcloud = WordCloud(width=800, height=800, background_color='white',font_path=font_path).generate(text)

plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("Word cloud of depressed users' tweets")

from collections import Counter
# Get word frequencies
words = wordcloud.process_text(text)
# Get the 10 most frequent words and their frequencies
most_common_words = Counter(words).most_common(10)

# Print the 10 most frequent words and their frequencies
print("Most frequent words and their frequencies:")
for word, frequency in most_common_words:
    print(f"{word}: {frequency}")

# Create a word cloud for label 0
text_label_0 = ' '.join(map(str, df[df['label'] == 0]['tweet_content']))
font_path = r'C:\Windows\Fonts\simfang.ttf'  # Set the font path for displaying Chinese characters
wordcloud_label_0 = WordCloud(width=800, height=800, background_color='white', font_path=font_path).generate(text_label_0)

# Get the word frequencies for label 0
word_frequencies_label_0 = wordcloud_label_0.process_text(text_label_0)
sorted_word_frequencies_label_0 = sorted(word_frequencies_label_0.items(), key=lambda x: x[1], reverse=True)

# Print the 10 most frequent words and their frequencies for label 0
print("10 Most Frequent Words in Normal Users' Tweets:")
for word, frequency in sorted_word_frequencies_label_0[:10]:
    print(f"{word}: {frequency}")

# Plot the word cloud for label 0
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud_label_0)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("Word Cloud of Normal Users' Tweets")

# Show the word cloud for label 0
plt.show()

# Create a word cloud for label 1
text_label_1 = ' '.join(map(str, df[df['label'] == 1]['tweet_content']))
wordcloud_label_1 = WordCloud(width=800, height=800, background_color='white', font_path=font_path).generate(text_label_1)

# Get the word frequencies for label 1
word_frequencies_label_1 = wordcloud_label_1.process_text(text_label_1)
sorted_word_frequencies_label_1 = sorted(word_frequencies_label_1.items(), key=lambda x: x[1], reverse=True)

# Print the 10 most frequent words and their frequencies for label 1
print("10 Most Frequent Words in Depressed Users' Tweets:")
for word, frequency in sorted_word_frequencies_label_1[:10]:
    print(f"{word}: {frequency}")

# Plot the word cloud for label 1
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud_label_1)
plt.axis("off")
plt.tight_layout(pad=0)
plt.title("Word Cloud of Depressed Users' Tweets")

# Show the word cloud for label 1
plt.show()

# Print the 30 most frequent words and their frequencies for label 0
print("30 Most Frequent Words in Normal Users' Tweets:")
for word, frequency in sorted_word_frequencies_label_0[:30]:
    print(f"{word}: {frequency}")

# Print the 30 most frequent words and their frequencies for label 1
print("30 Most Frequent Words in Depressed Users' Tweets:")
for word, frequency in sorted_word_frequencies_label_1[:30]:
    print(f"{word}: {frequency}")

# mode;s result

models_results= pd.read_csv('models_results.csv')

# Use the round() function to retain three decimal places
models_results = models_results.round(3)

models_results

# Set 'Model' column as the index
models_results.set_index('Model', inplace=True)


# Plot the bar chart and table
fig, ax = plt.subplots(figsize=(10, 6))

# Plot the bar chart
models_results.plot(kind='bar', rot=0, ax=ax)
plt.title('Performance Comparison of Models')
plt.xlabel('Models')
plt.ylabel('Score')
plt.legend(title='Metrics', bbox_to_anchor=(1, 1))

plt.tight_layout()
plt.show()
