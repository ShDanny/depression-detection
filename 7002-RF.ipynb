{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f104497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\lenovo\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.446 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# data clean\n",
    "import re # punctuation,data clean\n",
    "\n",
    "# tokenization and feature extraction\n",
    "# install expacted word bag for jieba\n",
    "import jieba \n",
    "jieba.load_userdict('jieba_expanded_dict.txt')\n",
    "import jieba.analyse # TF-IDF \n",
    "import numpy as np # TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier # RF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.neural_network import MLPClassifier # MLP\n",
    "from sklearn.model_selection import RandomizedSearchCV # MLP\n",
    "\n",
    "# analyse\n",
    "import seaborn as sns # density curve \n",
    "import matplotlib.pyplot as plt # pie chart\n",
    "\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6030ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df_all_cleaned_tokenization = pd.read_csv('balanced_df_all_cleaned_tokenization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e162c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df_all_cleaned_tokenization2 = balanced_df_all_cleaned_tokenization.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae3ee070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25640</td>\n",
       "      <td>0</td>\n",
       "      <td>害真 挺 吃 颜想 岁 足 表达 减肥 决心 姐妹 谈恋爱 感觉 全世界 闺蜜 介绍 成功 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28303</td>\n",
       "      <td>0</td>\n",
       "      <td>拥抱 世界 暖心 动作 生活 中 难免 挫折 坎坷 安 慌乱 许 拥抱 充满 量 世界 抗衡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10546</td>\n",
       "      <td>0</td>\n",
       "      <td>雨天 穿 滑倒 坐水里 亲测 艾特子 赟 家 意见 麻雀 妈妈 问 麻雀 扎 头发 麻雀 说...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26929</td>\n",
       "      <td>0</td>\n",
       "      <td>幸福 做 桌菜 然家 朋友 吃 完 没 帮忙 刷碗 老规矩 昨晚 想 做 早餐 奶包 发酵 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31080</td>\n",
       "      <td>0</td>\n",
       "      <td>日 清明 青草 疫情 中 牺牲 医护 员 公安干警 基层干部 线 工作 逝世 胞 沉痛 悼念...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20645</th>\n",
       "      <td>10321</td>\n",
       "      <td>1</td>\n",
       "      <td>意做件 错事 真 久久 原谅 近 发生 事 情绪 波动 房间 爆哭然 雨天 跑 出 外面 冻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20646</th>\n",
       "      <td>10322</td>\n",
       "      <td>1</td>\n",
       "      <td>太 喜欢 张 教授 说话 特实 危言耸听 然 整体 医生 亲戚朋友 医生 医生 子女 微博 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20647</th>\n",
       "      <td>10323</td>\n",
       "      <td>1</td>\n",
       "      <td>毒药 断送 爱 听说 爱情 蜜糖 深陷 沉醉 岂知 份 爱情 美意 想 原理 结束 意想 称...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20648</th>\n",
       "      <td>10324</td>\n",
       "      <td>1</td>\n",
       "      <td>果 原谅 痛苦 值 降低 贼 悔 吃 麻辣 串 洗完 澡 新 换衣服 全味 难受 新闻 男 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20649</th>\n",
       "      <td>10325</td>\n",
       "      <td>1</td>\n",
       "      <td>面 兽心 衣冠禽兽 说 求助 警方 包庇 社真药 救希 正义 早日 坏 绳法 回 分装 爱朵...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20650 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  label                                      tweet_content\n",
       "0      25640      0  害真 挺 吃 颜想 岁 足 表达 减肥 决心 姐妹 谈恋爱 感觉 全世界 闺蜜 介绍 成功 ...\n",
       "1      28303      0  拥抱 世界 暖心 动作 生活 中 难免 挫折 坎坷 安 慌乱 许 拥抱 充满 量 世界 抗衡...\n",
       "2      10546      0  雨天 穿 滑倒 坐水里 亲测 艾特子 赟 家 意见 麻雀 妈妈 问 麻雀 扎 头发 麻雀 说...\n",
       "3      26929      0  幸福 做 桌菜 然家 朋友 吃 完 没 帮忙 刷碗 老规矩 昨晚 想 做 早餐 奶包 发酵 ...\n",
       "4      31080      0  日 清明 青草 疫情 中 牺牲 医护 员 公安干警 基层干部 线 工作 逝世 胞 沉痛 悼念...\n",
       "...      ...    ...                                                ...\n",
       "20645  10321      1  意做件 错事 真 久久 原谅 近 发生 事 情绪 波动 房间 爆哭然 雨天 跑 出 外面 冻...\n",
       "20646  10322      1  太 喜欢 张 教授 说话 特实 危言耸听 然 整体 医生 亲戚朋友 医生 医生 子女 微博 ...\n",
       "20647  10323      1  毒药 断送 爱 听说 爱情 蜜糖 深陷 沉醉 岂知 份 爱情 美意 想 原理 结束 意想 称...\n",
       "20648  10324      1  果 原谅 痛苦 值 降低 贼 悔 吃 麻辣 串 洗完 澡 新 换衣服 全味 难受 新闻 男 ...\n",
       "20649  10325      1  面 兽心 衣冠禽兽 说 求助 警方 包庇 社真药 救希 正义 早日 坏 绳法 回 分装 爱朵...\n",
       "\n",
       "[20650 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_df_all_cleaned_tokenization2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f84c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ID  label                                      tweet_content\n",
      "10148  31839      0  手中 行 原油 宝 失误 客户 买单 国油板 价想 生日 月 日 祝福 事 花开花落 难堪 ...\n",
      "1692   22295      0  北极 绒纯 棉裤 条卷 评拍 送条 共 发条 肠道 提高 孩子 免疫力 卷 评晚 睡觉 抱 ...\n",
      "2692   17168      0  早安 周季 松茸 价格低 家斤 囤 优惠 特价 家 囤 祝 生日快乐 喝点 水朵 姣 云南 ...\n",
      "7703   21714      0  式样 友纸 召唤 训导 快 平安 京 富豪 灯烬 熄烛 阴阳 全新 阶式 神 浮世 青行 灯...\n",
      "321    29150      0  做 学习型 妈妈 建设 学习 成长型 家庭 建立 家庭 精神 宝藏 免费 围观 回答 价值 ...\n",
      "...      ...    ...                                                ...\n",
      "13388   3064      1  十 症状 否患 抑郁症 情绪 抑郁 抑郁 情绪 泣 成声 抑郁症 症状 然 抑郁症 患 感觉...\n",
      "17587   7263      1  确保 勇气 步原 轨迹 走次 想 杀 想 爸爸 难 告诉 熬 熬 熬 熬 睡觉 件 深刻 体...\n",
      "13695   3371      1  红枣 熬汤 止 润肺 取 银耳 克 红枣 颗加 冰糖 煮 半时 红枣 煮蛋 补血 养颜 红枣...\n",
      "13812   3488      1  悔 万分 难受 找 动摇 重蹈覆辙 想 原谅 周末 忙刻 闲 整场 活动 撑气 想 骂 洞察...\n",
      "10636    312      1  班族 晚班 步行 回家 间歇 跳绳 分钟 想 减肥 女性 摇呼圈 分钟 晚 仰卧 坐 男性 ...\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 取50行label为0，50行label为1\n",
    "df_label_0 = balanced_df_all_cleaned_tokenization2[balanced_df_all_cleaned_tokenization2['label'] == 0].sample(n=50, random_state=42)\n",
    "df_label_1 = balanced_df_all_cleaned_tokenization2[balanced_df_all_cleaned_tokenization2['label'] == 1].sample(n=50, random_state=42)\n",
    "\n",
    "# 合并两个dataframe\n",
    "balanced_df_all_cleaned_tokenization2 = pd.concat([df_label_0, df_label_1])\n",
    "\n",
    "print(balanced_df_all_cleaned_tokenization2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74bc36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values with an empty string\n",
    "balanced_df_all_cleaned_tokenization2['tweet_content'] = balanced_df_all_cleaned_tokenization2['tweet_content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ad8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TfidfVectorizer object 将tweet_content列作为特征向量化\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b68879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the data\n",
    "X = vectorizer.fit_transform(balanced_df_all_cleaned_tokenization2['tweet_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70af7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将label列作为目标变量\n",
    "y = balanced_df_all_cleaned_tokenization2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecaab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab628e4",
   "metadata": {},
   "source": [
    "## test e_estimatiors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6493c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个随机森林分类器\n",
    "rfc = RandomForestClassifier(random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b6c4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "Accuracy: [0.7 1.  0.9 1.  0.8 1.  0.9 0.9 0.9 1. ]\n",
      "Precision: [1.  1.  1.  1.  0.8 1.  1.  1.  1.  1. ]\n",
      "Recall: [0.4 1.  0.8 1.  0.8 1.  0.8 0.8 0.8 1. ]\n",
      "F1: [0.57142857 1.         0.88888889 1.         0.8        1.\n",
      " 0.88888889 0.88888889 0.88888889 1.        ]\n",
      "Average Accuracy: 0.9099999999999999\n",
      "Average Precision: 0.9800000000000001\n",
      "Average Recall: 0.8400000000000001\n",
      "Average F1 Score: 0.8926984126984128\n"
     ]
    }
   ],
   "source": [
    "# 使用cross_validate进行交叉验证，并计算多个评估指标，此时不需要划分测试集和训练集\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "cv_results = cross_validate(rfc, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# 输出每次交叉验证的结果\n",
    "print(\"Cross-validation results:\")\n",
    "for metric in scoring:\n",
    "    print(f\"{metric.capitalize()}: {cv_results[f'test_{metric}']}\")\n",
    "\n",
    "# 输出平均结果\n",
    "print(\"Average Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Average Precision:\", cv_results['test_precision'].mean())\n",
    "print(\"Average Recall:\", cv_results['test_recall'].mean())\n",
    "print(\"Average F1 Score:\", cv_results['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390d20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用10折交叉验证来确定n_estimators的最佳值\n",
    "scores = []\n",
    "for n in range(1, 101):\n",
    "    rf.set_params(n_estimators=n)\n",
    "    score = cross_val_score(rf, X_train, y_train, cv=10).mean()\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0022ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到具有最佳性能的模型\n",
    "best_n = np.argmax(scores) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9753ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用测试集来评估具有最佳性能的模型的泛化能力\n",
    "rf.set_params(n_estimators=best_n)\n",
    "rf.fit(X_train, y_train)\n",
    "test_score = rf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41fd90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best n_estimators value is: 13\n"
     ]
    }
   ],
   "source": [
    "# Print out the best n_estimators value\n",
    "print(\"The best n_estimators value is:\", best_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17726985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0816ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9154256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7f8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bdc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617b87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
